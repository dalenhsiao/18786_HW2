{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyNN import *\n",
    "from torchUtil import *\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            inputDim, \n",
    "            outputDim, \n",
    "            hiddenLayers=[128, 128], \n",
    "            opt_act=\"ReLU\", \n",
    "            opt_init=\"xavier\"\n",
    "            ):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.layers = [] # storing the model struture\n",
    "        self.opt_act = opt_act.lower()\n",
    "        self.opt_init = opt_init.lower()\n",
    "        self.gain = nn.init.calculate_gain(self.opt_act)# calculate the gain for parameters initialization \n",
    "        \n",
    "        for i in range(len(hiddenLayers)+1):\n",
    "            if i == 0:\n",
    "                layer = nn.Linear(inputDim, hiddenLayers[0])\n",
    "                \n",
    "            elif i == len(hiddenLayers):\n",
    "                layer = nn.Linear(hiddenLayers[-1], outputDim)\n",
    "                self._init_weights(layer)\n",
    "                self.layers.append(layer)\n",
    "                break \n",
    "            else:\n",
    "                layer = nn.Linear(hiddenLayers[i-1], hiddenLayers[i])\n",
    "                \n",
    "\n",
    "            self._init_weights(layer) # apply initialization\n",
    "            self.layers.append(layer) # append layer\n",
    "                \n",
    "\n",
    "\n",
    "            # Activations \n",
    "            if opt_act == \"ReLU\":\n",
    "                self.layers.append(nn.ReLU())\n",
    "\n",
    "            elif opt_act == \"Sigmoid\":\n",
    "                self.layers.append(nn.Sigmoid())\n",
    "            elif opt_act == \"tanh\":\n",
    "                self.layers.append(nn.Tanh())\n",
    "            elif opt_act == \"Linear\":\n",
    "                pass\n",
    "            else:\n",
    "                self.layers.append(nn.ReLU())\n",
    "                print(\"Invalid activation, automatically used ReLU as default\")  \n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "\n",
    "    # Parameters initialization \n",
    "    def _init_weights(self, layer):\n",
    "        if self.opt_init == \"xavier\": # xavier initialization \n",
    "            nn.init.xavier_uniform_(layer.weight, gain=self.gain)\n",
    "        elif self.opt_init == \"he\" or self.opt_init == \"kaiming\": # he initilaization \n",
    "            nn.init.kaiming_uniform_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "                nn.init.zeros_(layer.bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(10, 2, [50,50])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP():\n",
    "    def __init__(self, \n",
    "            structures,\n",
    "            opt_act=\"ReLU\", \n",
    "            opt_init=\"xavier\",\n",
    "            seed = 0\n",
    "            ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            structures (iterable): MLP struture in orderly fashion (including input and output)\n",
    "            opt_act (str, optional): Activation function. Daults to \"ReLU\".\n",
    "            opt_init (str, optional): Initialization methods. Defaults to \"xavier\".\n",
    "            seed (int, optional): Random seed. Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.structures = structures\n",
    "        self.weights = dict() # idx = order of layer - 1\n",
    "        self.biases = dict()\n",
    "        self.model = {\"W\":self.weights, \"b\": self.biases} # the entire model structure\n",
    "        self.opt_act = opt_act.lower()\n",
    "        self.opt_init = opt_init.lower()\n",
    "        self.seed = seed\n",
    "        self.cache = dict() # store the forward pass values\n",
    "        \n",
    "    # model training \n",
    "    def train(self, training_data, num_epoch, opt_loss, opt_optim):\n",
    "        if len(self.weights) == 0:\n",
    "            self.initialize_mlp()\n",
    "        \n",
    "\n",
    "    \n",
    "    # forward pass \n",
    "    def forward(self, x):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (numpy array): input data\n",
    "        \"\"\"\n",
    "        for i in range(len(self.weights)): \n",
    "            w, b = self.weights[str(i)], self.biases[str(i)]\n",
    "            u = np.dot(w,x) + b\n",
    "            if i == len(self.weights)-1: # skip the last layer (linear at the last layer)\n",
    "                y = u\n",
    "                self.cache[str(i)] = y\n",
    "                break\n",
    "            if self.opt_act == \"relu\":\n",
    "                y = self.ReLU(u)\n",
    "            elif self.opt_act == \"tanh\":\n",
    "                y = self.tanh(u)\n",
    "            elif self.opt_act == \"sigmoid\":\n",
    "                y = self.Sigmoid(u)\n",
    "            elif self.opt_act == \"linear\":\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Invalid activation, automatically use ReLU initialization\")\n",
    "                y = self.ReLU(u)\n",
    "\n",
    "            self.cache[str(i)] = y\n",
    "\n",
    "        return self.cache[str(len(self.weights)-1)] # return the values of final output layer\n",
    "\n",
    "    \n",
    "    # back propagation \n",
    "    def backwards(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # loss functions \n",
    "    def l2_loss(self,pred, label):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            pred (numpy array): model predictions\n",
    "            label (numpy array): ground truth labels\n",
    "        \"\"\"\n",
    "        pred = pred.reshape((pred.shape[0], -1))\n",
    "        label = label.reshape((label.shape[0], -1))\n",
    "        loss = np.sum(np.square(label-pred))\n",
    "        return loss\n",
    "    \n",
    "    def cross_entropy(self,pred, label):\n",
    "        pred = self.softmax(pred).reshape((pred.shape[0], -1)) # apply softmax function\n",
    "        label = label.reshape((label.shape[0], -1))\n",
    "        loss = -np.sum(label * np.log(pred))\n",
    "        return loss\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    # model parameters initialization \n",
    "    def initialize_mlp(self):\n",
    "        np.random.seed(self.seed)\n",
    "        for i in range(len(self.structures)-1):\n",
    "            self.weights[str(i)], self.biases[str(i)] = self._init_single_layer(self.structures[i], self.structures[i+1])\n",
    "        print(\"Successfully initilaize MLP\")\n",
    "        return self.model\n",
    "    \n",
    "    # parameters initialization of a single layer perceptrons\n",
    "    def _init_single_layer(self, in_shape, out_shape):\n",
    "        if self.opt_init == \"xavier\":\n",
    "            weight = np.random.randn(out_shape, in_shape)*np.sqrt(1. / in_shape)\n",
    "        elif self.opt_init == \"he\" or self.opt_init == \"kaiming\":\n",
    "            weight = np.random.randn(out_shape, in_shape)*np.sqrt(2. / in_shape)\n",
    "        else:\n",
    "            print(\"Invalid initialization method, automatically use xavier initialization\")\n",
    "            weight = np.random.randn(out_shape, in_shape)*np.sqrt(1. / in_shape)\n",
    "        bias = np.zeros((out_shape,1))\n",
    "        return weight, bias\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Activation functions\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    def Sigmoid(self,x):\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    def Linear(self,x):\n",
    "        return x\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([2,10,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initilaize MLP\n",
      "{'W': {'0': array([[ 1.24737338,  0.28295388],\n",
      "       [ 0.69207227,  1.58455078],\n",
      "       [ 1.32056292, -0.69103982],\n",
      "       [ 0.67181396, -0.10702571],\n",
      "       [-0.07298675,  0.29033699],\n",
      "       [ 0.10185419,  1.02832666],\n",
      "       [ 0.53813494,  0.08603723],\n",
      "       [ 0.3138587 ,  0.23594338],\n",
      "       [ 1.05647344, -0.1450688 ],\n",
      "       [ 0.22137229, -0.60393689]]), '1': array([[-0.80732627,  0.20669235,  0.27335873, -0.23469319,  0.71775943,\n",
      "        -0.45991081,  0.01447011, -0.05919273,  0.48470735,  0.46465204],\n",
      "       [ 0.04899868,  0.11958549, -0.2807425 , -0.62638284, -0.11001948,\n",
      "         0.04944189,  0.38905207,  0.38022589, -0.12248349, -0.09559652],\n",
      "       [-0.33158156, -0.4490491 , -0.53957001,  0.61688935, -0.16116617,\n",
      "        -0.13853126, -0.39616868,  0.24586404, -0.51035931, -0.06727438],\n",
      "       [-0.28317139,  0.12234931, -0.16153077, -0.37334868, -0.008912  ,\n",
      "         0.13545043,  0.02103459,  0.09565001, -0.20059026, -0.11470883],\n",
      "       [-0.21265067, -0.11370069, -0.25713943, -0.54589849,  0.05610707,\n",
      "        -0.12705429, -0.51551398,  0.1463446 , -0.28691293,  0.01642658]])}, 'b': {'0': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), '1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])}}\n"
     ]
    }
   ],
   "source": [
    "mlp = model.initialize_mlp()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {str(i):[] for i in range(3)}\n",
    "a is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = dict()\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
